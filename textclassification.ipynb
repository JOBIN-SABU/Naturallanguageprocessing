{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9d83cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b1009fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"C:\\Users\\85job\\OneDrive\\Desktop\\textclassification\\train.csv\",encoding='ISO-8859-1')\n",
    "test_df = pd.read_csv(r\"C:\\Users\\85job\\OneDrive\\Desktop\\textclassification\\train.csv\",encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10a89e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       textID                                               text  \\\n",
      "0  cb774db0d1                I`d have responded, if I were going   \n",
      "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2  088c60f138                          my boss is bullying me...   \n",
      "3  9642c003ef                     what interview! leave me alone   \n",
      "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "\n",
      "                         selected_text sentiment Time of Tweet Age of User  \\\n",
      "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
      "1                             Sooo SAD  negative          noon       21-30   \n",
      "2                          bullying me  negative         night       31-45   \n",
      "3                       leave me alone  negative       morning       46-60   \n",
      "4                        Sons of ****,  negative          noon       60-70   \n",
      "\n",
      "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
      "0  Afghanistan          38928346         652860.0               60  \n",
      "1      Albania           2877797          27400.0              105  \n",
      "2      Algeria          43851044        2381740.0               18  \n",
      "3      Andorra             77265            470.0              164  \n",
      "4       Angola          32866272        1246700.0               26  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f03b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['sentiment'] = label_encoder.fit_transform(train_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93224d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-preprocessing in c:\\users\\85job\\anaconda3\\lib\\site-packages (1.1.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\85job\\anaconda3\\lib\\site-packages (from keras-preprocessing) (1.23.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\85job\\anaconda3\\lib\\site-packages (from keras-preprocessing) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install keras-preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02c07c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a26cee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=10000) \n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "\n",
    "\n",
    "max_sequence_length = 100  \n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd17a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    padded_sequences, train_df['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f4dce12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\85job\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_sequence_length),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax') \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c4c24d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.4906 - loss: 0.9852 - val_accuracy: 0.6796 - val_loss: 0.7434\n",
      "Epoch 2/5\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - accuracy: 0.7858 - loss: 0.5510 - val_accuracy: 0.6889 - val_loss: 0.7504\n",
      "Epoch 3/5\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.9108 - loss: 0.2666 - val_accuracy: 0.6538 - val_loss: 0.9756\n",
      "Epoch 4/5\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.9705 - loss: 0.1062 - val_accuracy: 0.6549 - val_loss: 1.1908\n",
      "Epoch 5/5\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - accuracy: 0.9858 - loss: 0.0534 - val_accuracy: 0.6464 - val_loss: 1.4108\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9598b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6380 - loss: 1.4428\n",
      "Validation Loss: 1.4108315706253052\n",
      "Validation Accuracy: 0.6463525295257568\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Loss: {loss}')\n",
    "print(f'Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31fe66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
